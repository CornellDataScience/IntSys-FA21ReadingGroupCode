from torch.optim import Optimizer
import torch
class ADAMOptimizer(Optimizer):
    def __init__(self, params, lr=0.001, betas=(0.9, 0.99), eps=1e-8, weight_decay=0):
        #   Require: Î±: Stepsize
        #   Require: Î²1, Î²2 âˆˆ [0, 1): Exponential decay rates for the moment estimates
        #   Require: f(Î¸): Stochastic objective function with parameters Î¸
        #   Require: Î¸0: Initial parameter vector
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        super(ADAMOptimizer, self).__init__(params, defaults)
        # structure stuff when it implements Optimizer
        for group in self.param_groups:
            for p in group['params']:
                 state = self.state[p]
                 # m0 â† 0 (Initialize 1st moment vector)
                 state['m'] = torch.zeros_like(p, memory_format=torch.preserve_format)

                 #v0 â† 0 (Initialize 2nd moment vector)
                 state['v'] = torch.zeros_like(p, memory_format=torch.preserve_format)

                 #t â† 0 (Initialize timestep)
                 state['t'] = 0

    #while Î¸t not converged do
    def step(self):
        for group in self.param_groups:
            for p in group['params']:
                state = self.state[p]
                state['t']+=1
                # Grad decay stuff to fit with original adam format for optmiizer
                if group['weight_decay'] != 0:
                    grad = grad.add(group['weight_decay'], p.data)
                b1, b2 = group['betas']

                #gt â† âˆ‡Î¸ft(Î¸tâˆ’1) (Get gradients w.r.t. stochastic objective at timestep t)
                #gt is already there with autograd in p.grad.data variable

                #mt â† Î²1 Â· mtâˆ’1 + (1 âˆ’ Î²1) Â· gt (Update biased first moment estimate)
                state['m'] = torch.mul(b1, state['m']) + (1-b1) * p.grad.data

                #vt â† Î²2 Â· vtâˆ’1 + (1 âˆ’ Î²2) Â· g2t (Update biased second raw moment estimate)
                state['v'] = torch.mul(b2, state['v']) + (1-b2) * (p.grad.data * p.grad.data)

                #mb t â† mt/(1 âˆ’ Î²t1) (Compute bias-corrected first moment estimate)
                m_hat = state['m']/(1-b1**state['t'])

                #vbt â† vt/(1 âˆ’ Î²t2) (Compute bias-corrected second raw moment estimate)
                v_hat = state['v']/(1-b2**state['t'])

                #Î¸t â† Î¸tâˆ’1 âˆ’ Î± Â· mb t/(âˆšvbt + ) (Update parameters)
                p.data = p.data - group['lr'] * m_hat / (v_hat.sqrt()+group['eps'])

    #end while
